{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "filedir = os.path.abspath(r\"C:\\Users\\BABI\\Dynamic Blog Recommendation\\Sample Data\")\n",
    "medium_filename = \"medium_final.csv\"\n",
    "toward_filename = \"towards_data_science.csv\"\n",
    "analytics_filename= \"analytics_vidya.csv\"\n",
    "filepath_medium = os.path.join(filedir, medium_filename)\n",
    "filepath_toward = os.path.join(filedir, toward_filename)\n",
    "filepath_analytics = os.path.join(filedir, analytics_filename)\n",
    "raw_data_medium = pa.read_csv(filepath_medium)\n",
    "raw_data_toward = pa.read_csv(filepath_toward)\n",
    "raw_data_analytics = pa.read_csv(filepath_analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For removing emojis from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 1.20 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\BABI\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import demoji \n",
    "demoji.download_codes() #(Required first time )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [\"\", \"one \",\"two \",\"three \",\"four \", \"five \", \"six \",\"seven \",\"eight \",\"nine \",\"ten \",\"eleven \",\n",
    "        \"twelve \", \"thirteen \", \"fourteen \", \"fifteen \",\"sixteen \",\"seventeen \", \"eighteen \",\"nineteen \"]\n",
    "twenties = [\"\",\"\",\"twenty \",\"thirty \",\"forty \", \"fifty \",\"sixty \",\"seventy \",\"eighty \",\"ninety \"]\n",
    "thousands = [\"\",\"thousand \",\"million \", \"billion \", \"trillion \", \"quadrillion \", \"quintillion \", \"sextillion \",\n",
    "             \"septillion \",\"octillion \", \"nonillion \", \"decillion \", \"undecillion \", \"duodecillion \", \"tredecillion \",\n",
    "             \"quattuordecillion \", \"quindecillion\", \"sexdecillion \", \"septendecillion \", \"octodecillion \", \"novemdecillion \",\n",
    "             \"vigintillion \"]\n",
    "def num999(n):\n",
    "    c = int(n % 10) # singles digit\n",
    "    b = int(((n % 100) - c) / 10) # tens digit\n",
    "    a = int(((n % 1000) - (b * 10) - c) / 100) # hundreds digit\n",
    "    t = \"\"\n",
    "    h = \"\"\n",
    "    if a != 0 and b == 0 and c == 0:\n",
    "        t = ones[a] + \"hundred \"\n",
    "    elif a != 0:\n",
    "        t = ones[a] + \"hundred and \"\n",
    "    if b <= 1:\n",
    "        h = ones[n%100]\n",
    "    elif b > 1:\n",
    "        h = twenties[b] + ones[c]\n",
    "    st = t + h\n",
    "    return st\n",
    "def num2word(num):\n",
    "    if num == 0: return 'zero'\n",
    "    i = 3\n",
    "    n = str(num)\n",
    "    word = \"\"\n",
    "    k = 0\n",
    "    while(i == 3):\n",
    "        nw = n[-i:]\n",
    "        n = n[:-i]\n",
    "        if int(nw) == 0:\n",
    "            word = num999(int(nw)) + thousands[int(nw)] + word\n",
    "        else:\n",
    "            word = num999(int(nw)) + thousands[k] + word\n",
    "        if n == '':\n",
    "            i = i+1\n",
    "        k += 1\n",
    "    return word[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subs(word):\n",
    "    number = word.group(0)\n",
    "    return (num2word(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_numbers_percent(word):\n",
    "    return re.sub('[\\d]+',subs,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"we'll\" : \"we will\"   ,\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "lem = WordNetLemmatizer()\n",
    "engstopwords = (stopwords.words('english'))\n",
    "punc = string.punctuation\n",
    "\n",
    "extra_stopwords = ['important' ,'ai' ,'ml' ,'blackbelt' ,'program' ,'enrollments','open','seventh','april']\n",
    "for word in extra_stopwords:\n",
    "    engstopwords.append(word)\n",
    "\n",
    "engstopwords = set(engstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = {'):','(:',':',':-)',':))'} #For removing bracket emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    #Remove email\n",
    "    re_email = re.compile(r'[\\w.-]+@[\\w.-]+')\n",
    "    data= re_email.sub(r'',data)\n",
    "    #Remove Emoji\n",
    "    data = demoji.replace(data,repl='')\n",
    "    #Remove webiste\n",
    "    reg_website = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([\\w+-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    data= reg_website.sub(r'',data)\n",
    "    #convert lower case\n",
    "    data = data.lower()\n",
    "    #convert numbers to words\n",
    "    data = find_numbers_percent(data)\n",
    "    #convert percent\n",
    "    data = re.sub(r'%','percent',data)\n",
    "    data = re.sub(r'’',\"'\",data)\n",
    "    data = re.sub(r'\\.',\"\",data)\n",
    "    data = re.sub(r'-',\"\",data)\n",
    "    data = re.sub(r'–',\"\",data)\n",
    "    data = re.sub(r'”',\"\",data)\n",
    "    data = re.sub(r':',\"\",data)\n",
    "    data = re.sub(r'‘',\"\",data)\n",
    "    data = re.sub(r'“',\"\",data)\n",
    "    # tokenize words\n",
    "    words = tokenizer.tokenize(data)\n",
    "    # add appos\n",
    "    words = [APPO[word] if word in APPO else word for word in words]\n",
    "    # stop words\n",
    "    words = [word for word in words if not word in engstopwords]\n",
    "    words = [lem.lemmatize(word,'v') for word in words]\n",
    "    #Remove Punctuanitions\n",
    "    #clean_data = [str('') if word == '.' else word for word in words]\n",
    "    clean_data = [word for word in words if word not in punc]\n",
    "    #clean_data = [word for word in words if wor]\n",
    "    clean_data = [word for word in clean_data if word not in emo]\n",
    "    cleaned_data =  \" \".join(clean_data)\n",
    "    \n",
    "    #cleaned_data = re.sub(r'.','dot',cleaned_data)\n",
    "    cleaned_data = re.sub(r\"'\",'',cleaned_data)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_toward['Description'] = raw_data_toward['Description'].apply(lambda x:data_cleaning(x))\n",
    "raw_data_medium['Description'] = raw_data_medium['Description'].apply(lambda x:data_cleaning(x))\n",
    "raw_data_analytics['Description'] = raw_data_analytics['Description'].apply(lambda x:data_cleaning(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_toward.to_csv('cleaned_towards_data_science')\n",
    "raw_data_medium.to_csv('cleaned_medium')\n",
    "raw_data_analytics.to_csv('cleaned_analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
